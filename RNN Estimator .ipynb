{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Import Data Tensorflow SnippetsÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version  1.12.0\n",
      "pandas version  0.23.4\n",
      "numpy version  1.15.4\n"
     ]
    }
   ],
   "source": [
    "print('tensorflow version ',tf.__version__)\n",
    "print('pandas version ',pd.__version__)\n",
    "print('numpy version ', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Data Gathering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "import tempfile\n",
    "\n",
    "class movieReviewData():\n",
    "\n",
    "#vocab_size = 5000\n",
    "#embedding_size = 50\n",
    "#sentence_size=200\n",
    "#start_id = 1\n",
    "#oov_id = 2\n",
    "#index_offset = 2\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vocab_size = 5000\n",
    "        self.start_id = 1\n",
    "        self.oov_id = 2\n",
    "        self.index_offset = 2\n",
    "        self.sentence_size = 200\n",
    "        \n",
    "        model_dir = tempfile.mkdtemp()  \n",
    "        \n",
    "        print(\"Loading data...\")\n",
    "        (self.x_train_variable, self.y_train), (self.x_test_variable, self.y_test) = imdb.load_data(\n",
    "            num_words=self.vocab_size, start_char=self.start_id, oov_char=self.oov_id,\n",
    "            index_from=self.index_offset)\n",
    "        \n",
    "        self.x_train = 0\n",
    "        self.x_test = 0\n",
    "        \n",
    "        print(len(self.y_train), \"train sequences\")\n",
    "        print(len(self.y_test), \"test sequences\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def preProcessing(self):\n",
    "        '''\n",
    "            Description: \n",
    "                - Load data\n",
    "                - Convert from text to index\n",
    "                - 0 post-padding \n",
    "            Usage:\n",
    "        '''\n",
    "        \n",
    "#        sentence_size = 200\n",
    "    #    embedding_size = 50\n",
    "    \n",
    "        \n",
    "        # we assign the first indices in the vocabulary to special tokens that we use\n",
    "        # for padding, as start token, and for indicating unknown words\n",
    "        \n",
    "        pad_id = 0\n",
    "    \n",
    "        print(\"Pad sequences (samples x time)\")\n",
    "        self.x_train = sequence.pad_sequences(self.x_train_variable, \n",
    "                                         maxlen=self.sentence_size,\n",
    "                                         truncating='post',\n",
    "                                         padding='post',\n",
    "                                         value=pad_id)\n",
    "        self.x_test = sequence.pad_sequences(self.x_test_variable, \n",
    "                                        maxlen=self.sentence_size,\n",
    "                                        truncating='post',\n",
    "                                        padding='post', \n",
    "                                        value=pad_id)\n",
    "        \n",
    "        print(\"x_train shape:\", self.x_train.shape)\n",
    "        print(\"x_test shape:\", self.x_test.shape)\n",
    "        \n",
    "\n",
    "    \n",
    "    def convert2Text(self,pad_id,oov_id,start_id,index_offset):\n",
    "        '''\n",
    "            Description: covert index to text\n",
    "            Usage:\n",
    "        '''\n",
    "        word_index = imdb.get_word_index()\n",
    "        word_inverted_index = {v + index_offset: k for k, v in word_index.items()}\n",
    "        \n",
    "        # The first indexes in the map are reserved to represent things other than tokens\n",
    "        word_inverted_index[pad_id] = '<PAD>'\n",
    "        word_inverted_index[start_id] = '<START>'\n",
    "        word_inverted_index[oov_id] = '<OOV>'\n",
    "        \n",
    "        for i in range(0, 10):\n",
    "          print(i, word_inverted_index[i])\n",
    "          \n",
    "        def index_to_text(indexes):\n",
    "            return ' '.join([word_inverted_index[i] for i in indexes])\n",
    "        \n",
    "        print(index_to_text(self.x_train_variable[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain data from above class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = movieReviewData()\n",
    "\n",
    "# Preprocessing data\n",
    "data.preProcessing()\n",
    "\n",
    "vocab_size = data.vocab_size\n",
    "embedding_size = 50\n",
    "sentence_size = data.sentence_size\n",
    "\n",
    "#     Prepare data\n",
    "x_train = data.x_train\n",
    "x_test = data.x_test\n",
    "\n",
    "y_train = data.y_train\n",
    "y_test  = data.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model function, holding RNN + LSTM Cells which is fed into Tensorflow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model_fn(features, labels, mode):\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "            features['x'],vocab_size,embed_dim=embedding_size,\n",
    "            initializer=tf.random_uniform_initializer(-1.0,-1.0))\n",
    "    \n",
    "    # create an LSTM cell of size 100\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)\n",
    "    \n",
    "    sequence_length = tf.count_nonzero(features['x'], 1)\n",
    "    \n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, inputs, sequence_length = sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    outputs = final_states.h   \n",
    "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "    \n",
    "    if labels is not None: # vertical array\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"next\": tf.round(tf.sigmoid(logits)),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      \"probabilities\": tf.sigmoid(logits, name=\"sigmoid_tensor\")\n",
    "      }\n",
    "\n",
    "    # Prediction\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,predictions=predictions[\"next\"])\n",
    "    \n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels,logits)\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(\n",
    "                    labels=labels, predictions=predictions[\"next\"])}\n",
    "    return tf.estimator.EstimatorSpec( mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x, y):\n",
    "    '''\n",
    "        Description: \n",
    "        Usage:\n",
    "    '''\n",
    "    \n",
    "    features = {\"x\": x}\n",
    "#    y_ = {\"next\":y}\n",
    "    return features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(x_train,y_train,batch_size):\n",
    "    '''\n",
    "        Description: \n",
    "        Usage:\n",
    "    '''\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size).map(parser).repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(x_train,y_train,batch_size):\n",
    "    '''\n",
    "        Description: \n",
    "        Usage:\n",
    "    '''\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "    dataset = dataset.batch(batch_size).map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \"\"\"\n",
    "        Description: This is used to define inputs to serve the model.\n",
    "        Usage:\n",
    "        return: ServingInputReciever\n",
    "        Ref: https://www.tensorflow.org/versions/r1.7/api_docs/python/tf/estimator/export/ServingInputReceiver\n",
    "    \"\"\"\n",
    "\n",
    "    reciever_tensors = {\n",
    "        # The size of input sentence is flexible.\n",
    "        \"sentence\":tf.placeholder(tf.int32, [None, 1])\n",
    "    }\n",
    "    \n",
    "  \n",
    "    features = {\n",
    "        # Resize given images.\n",
    "        \"x\": tf.reshape(reciever_tensors[\"sentence\"], [200, 1])\n",
    "    }\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(receiver_tensors=reciever_tensors,\n",
    "                                                    features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('model_dir', './models/ckpt/', 'Dir to save a model and checkpoints')\n",
    "tf.app.flags.DEFINE_string('saved_dir', './models/pb/', 'Dir to save a model for TF serving')\n",
    "#tf.app.flags.DEFINE_string('step_size', '20000', 'Step Size')\n",
    "tf.app.flags.DEFINE_string('step_size', '10', 'Step Size')\n",
    "#tf.app.flags.DEFINE_string('batch_size', './models/pb/', 'Batch Size')\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd0c1d18250>, '_model_dir': './models/ckpt/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From <ipython-input-13-bab04b98736d>:15: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./models/ckpt/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 10 into ./models/ckpt/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[0.5509896 ]\n",
      " [0.5514494 ]\n",
      " [0.5505963 ]\n",
      " [0.54978615]\n",
      " [0.55037194]\n",
      " [0.5517112 ]\n",
      " [0.55225545]\n",
      " [0.5507216 ]\n",
      " [0.55149645]\n",
      " [0.55195105]\n",
      " [0.5520187 ]\n",
      " [0.55061483]\n",
      " [0.551189  ]\n",
      " [0.55106384]\n",
      " [0.5505554 ]\n",
      " [0.5514374 ]\n",
      " [0.5507538 ]\n",
      " [0.5498276 ]\n",
      " [0.55183154]\n",
      " [0.5512828 ]\n",
      " [0.5510605 ]\n",
      " [0.5517384 ]\n",
      " [0.5498658 ]\n",
      " [0.5516229 ]\n",
      " [0.5517601 ]\n",
      " [0.54962426]\n",
      " [0.55092996]\n",
      " [0.55022275]\n",
      " [0.5504454 ]\n",
      " [0.55152154]\n",
      " [0.5506313 ]\n",
      " [0.5507999 ]\n",
      " [0.5516624 ]\n",
      " [0.54952836]\n",
      " [0.55259544]\n",
      " [0.5505293 ]\n",
      " [0.55125725]\n",
      " [0.5500414 ]\n",
      " [0.55094373]\n",
      " [0.5510566 ]\n",
      " [0.55121124]\n",
      " [0.55071163]\n",
      " [0.55046064]\n",
      " [0.55091804]\n",
      " [0.55145323]\n",
      " [0.55161697]\n",
      " [0.5497985 ]\n",
      " [0.54992634]\n",
      " [0.551255  ]\n",
      " [0.5511857 ]\n",
      " [0.55142146]\n",
      " [0.5497667 ]\n",
      " [0.5508789 ]\n",
      " [0.5502631 ]\n",
      " [0.5507877 ]\n",
      " [0.5494884 ]\n",
      " [0.5501086 ]\n",
      " [0.55169994]\n",
      " [0.5503776 ]\n",
      " [0.55092674]\n",
      " [0.55143857]\n",
      " [0.5501248 ]\n",
      " [0.5510979 ]\n",
      " [0.5514545 ]\n",
      " [0.55026543]\n",
      " [0.55160165]\n",
      " [0.55191636]\n",
      " [0.5514317 ]\n",
      " [0.5513462 ]\n",
      " [0.5510243 ]\n",
      " [0.55162185]\n",
      " [0.54999703]\n",
      " [0.55121434]\n",
      " [0.55191714]\n",
      " [0.54955906]\n",
      " [0.5514255 ]\n",
      " [0.54949844]\n",
      " [0.5512698 ]\n",
      " [0.5505243 ]\n",
      " [0.55083686]\n",
      " [0.55100435]\n",
      " [0.55025357]\n",
      " [0.5510079 ]\n",
      " [0.55078536]\n",
      " [0.54958844]\n",
      " [0.5500129 ]\n",
      " [0.54994494]\n",
      " [0.5505628 ]\n",
      " [0.55200326]\n",
      " [0.5494898 ]\n",
      " [0.54990935]\n",
      " [0.55150515]\n",
      " [0.5490229 ]\n",
      " [0.5517795 ]\n",
      " [0.5517074 ]\n",
      " [0.55158734]\n",
      " [0.5508799 ]\n",
      " [0.55109465]\n",
      " [0.5518731 ]\n",
      " [0.55107045]]\n",
      "INFO:tensorflow:loss = 0.68116724, step = 11\n",
      "INFO:tensorflow:Saving checkpoints for 20 into ./models/ckpt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6840741.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-12-21:52:54\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./models/ckpt/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-12-21:53:11\n",
      "INFO:tensorflow:Saving dict for global step 20: accuracy = 0.5, global_step = 20, loss = 0.69677734\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20: ./models/ckpt/model.ckpt-20\n",
      "{'loss': 0.69677734, 'global_step': 20, 'accuracy': 0.5}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from ./models/ckpt/model.ckpt-20\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py:1044: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./models/pb/temp-1544651592/saved_model.pb\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2886: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(unused_argv):\n",
    "    # Create the Estimator\n",
    "    RNN_classifier = tf.estimator.Estimator(model_fn=LSTM_model_fn, model_dir= FLAGS.model_dir)\n",
    "    \n",
    "    tensors_to_log = {\"probabilities\": \"sigmoid_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "    \n",
    "    STEP_SIZE = int(FLAGS.step_size)\n",
    "    \n",
    "    RNN_classifier.train(\n",
    "        input_fn= lambda: train_input_fn(x_train,y_train,batch_size=100),\n",
    "        steps=STEP_SIZE,\n",
    "        hooks=[logging_hook])  \n",
    "\n",
    "    eval_results = RNN_classifier.evaluate(\n",
    "       input_fn = lambda: eval_input_fn(x_test,y_test,batch_size=100))\n",
    "    \n",
    "    print(eval_results)\n",
    "    \n",
    "    # Save the model\n",
    "    RNN_classifier.export_savedmodel(FLAGS.saved_dir, serving_input_receiver_fn=serving_input_receiver_fn)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
