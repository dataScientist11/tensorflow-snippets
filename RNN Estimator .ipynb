{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Import Data Tensorflow SnippetsÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version  1.12.0\n",
      "pandas version  0.23.4\n",
      "numpy version  1.15.4\n"
     ]
    }
   ],
   "source": [
    "print('tensorflow version ',tf.__version__)\n",
    "print('pandas version ',pd.__version__)\n",
    "print('numpy version ', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Data Gathering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.python.keras.datasets import imdb\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "import tempfile\n",
    "\n",
    "class movieReviewData():\n",
    "\n",
    "#vocab_size = 5000\n",
    "#embedding_size = 50\n",
    "#sentence_size=200\n",
    "#start_id = 1\n",
    "#oov_id = 2\n",
    "#index_offset = 2\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vocab_size = 5000\n",
    "        self.start_id = 1\n",
    "        self.oov_id = 2\n",
    "        self.index_offset = 2\n",
    "        self.sentence_size = 200\n",
    "        \n",
    "        model_dir = tempfile.mkdtemp()  \n",
    "        \n",
    "        print(\"Loading data...\")\n",
    "        (self.x_train_variable, self.y_train), (self.x_test_variable, self.y_test) = imdb.load_data(\n",
    "            num_words=self.vocab_size, start_char=self.start_id, oov_char=self.oov_id,\n",
    "            index_from=self.index_offset)\n",
    "        \n",
    "        self.x_train = 0\n",
    "        self.x_test = 0\n",
    "        \n",
    "        print(len(self.y_train), \"train sequences\")\n",
    "        print(len(self.y_test), \"test sequences\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def preProcessing(self):\n",
    "        '''\n",
    "            Description: \n",
    "                - Load data\n",
    "                - Convert from text to index\n",
    "                - 0 post-padding \n",
    "            Usage:\n",
    "        '''\n",
    "        \n",
    "#        sentence_size = 200\n",
    "    #    embedding_size = 50\n",
    "    \n",
    "        \n",
    "        # we assign the first indices in the vocabulary to special tokens that we use\n",
    "        # for padding, as start token, and for indicating unknown words\n",
    "        \n",
    "        pad_id = 0\n",
    "    \n",
    "        print(\"Pad sequences (samples x time)\")\n",
    "        self.x_train = sequence.pad_sequences(self.x_train_variable, \n",
    "                                         maxlen=self.sentence_size,\n",
    "                                         truncating='post',\n",
    "                                         padding='post',\n",
    "                                         value=pad_id)\n",
    "        self.x_test = sequence.pad_sequences(self.x_test_variable, \n",
    "                                        maxlen=self.sentence_size,\n",
    "                                        truncating='post',\n",
    "                                        padding='post', \n",
    "                                        value=pad_id)\n",
    "        \n",
    "        print(\"x_train shape:\", self.x_train.shape)\n",
    "        print(\"x_test shape:\", self.x_test.shape)\n",
    "        \n",
    "\n",
    "    \n",
    "    def convert2Text(self,pad_id,oov_id,start_id,index_offset):\n",
    "        '''\n",
    "            Description: covert index to text\n",
    "            Usage:\n",
    "        '''\n",
    "        word_index = imdb.get_word_index()\n",
    "        word_inverted_index = {v + index_offset: k for k, v in word_index.items()}\n",
    "        \n",
    "        # The first indexes in the map are reserved to represent things other than tokens\n",
    "        word_inverted_index[pad_id] = '<PAD>'\n",
    "        word_inverted_index[start_id] = '<START>'\n",
    "        word_inverted_index[oov_id] = '<OOV>'\n",
    "        \n",
    "        for i in range(0, 10):\n",
    "          print(i, word_inverted_index[i])\n",
    "          \n",
    "        def index_to_text(indexes):\n",
    "            return ' '.join([word_inverted_index[i] for i in indexes])\n",
    "        \n",
    "        print(index_to_text(self.x_train_variable[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain data from above class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 200)\n",
      "x_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = movieReviewData()\n",
    "\n",
    "# Preprocessing data\n",
    "data.preProcessing()\n",
    "\n",
    "vocab_size = data.vocab_size\n",
    "embedding_size = 50\n",
    "sentence_size = data.sentence_size\n",
    "\n",
    "#     Prepare data\n",
    "x_train = data.x_train\n",
    "x_test = data.x_test\n",
    "\n",
    "y_train = data.y_train\n",
    "y_test  = data.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model function, holding RNN + LSTM Cells which is fed into Tensorflow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model_fn(features, labels, mode):\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "            features['x'],vocab_size,embed_dim=embedding_size,\n",
    "            initializer=tf.random_uniform_initializer(-1.0,-1.0))\n",
    "    \n",
    "    # create an LSTM cell of size 100\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(100)\n",
    "    \n",
    "    sequence_length = tf.count_nonzero(features['x'], 1)\n",
    "    \n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, inputs, sequence_length = sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    outputs = final_states.h   \n",
    "    logits = tf.layers.dense(inputs=outputs, units=1)\n",
    "    \n",
    "    if labels is not None: # vertical array\n",
    "        labels = tf.reshape(labels, [-1, 1])\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"next\": tf.round(tf.sigmoid(logits)),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      \"probabilities\": tf.sigmoid(logits, name=\"sigmoid_tensor\")\n",
    "      }\n",
    "\n",
    "    # Prediction\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,predictions=predictions[\"next\"])\n",
    "    \n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels,logits)\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(\n",
    "                    labels=labels, predictions=predictions[\"next\"])}\n",
    "    return tf.estimator.EstimatorSpec( mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x, y):\n",
    "    '''\n",
    "        Description: \n",
    "        Usage:\n",
    "    '''\n",
    "    \n",
    "    features = {\"x\": x}\n",
    "#    y_ = {\"next\":y}\n",
    "    return features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(x_train,y_train,batch_size):\n",
    "    '''\n",
    "        Description: \n",
    "        Usage:\n",
    "    '''\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size).map(parser).repeat()\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(x_train,y_train,batch_size):\n",
    "    '''\n",
    "        Description: \n",
    "        Usage:\n",
    "    '''\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "    dataset = dataset.batch(batch_size).map(parser)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \"\"\"\n",
    "        Description: This is used to define inputs to serve the model.\n",
    "        Usage:\n",
    "        return: ServingInputReciever\n",
    "        Ref: https://www.tensorflow.org/versions/r1.7/api_docs/python/tf/estimator/export/ServingInputReceiver\n",
    "    \"\"\"\n",
    "\n",
    "    reciever_tensors = {\n",
    "        # The size of input sentence is flexible.\n",
    "        \"sentence\":tf.placeholder(tf.int32, [None, 1])\n",
    "    }\n",
    "    \n",
    "  \n",
    "    features = {\n",
    "        # Resize given images.\n",
    "        \"x\": tf.reshape(reciever_tensors[\"sentence\"], [200, 1])\n",
    "    }\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(receiver_tensors=reciever_tensors,\n",
    "                                                    features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'FLAGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-357dc54ac375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-357dc54ac375>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munused_argv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Create the Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mRNN_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLSTM_model_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtensors_to_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"probabilities\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"sigmoid_tensor\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'FLAGS' is not defined"
     ]
    }
   ],
   "source": [
    "def main(unused_argv):\n",
    "    # Create the Estimator\n",
    "    RNN_classifier = tf.estimator.Estimator(model_fn=LSTM_model_fn, model_dir= FLAGS.model_dir)\n",
    "    \n",
    "    tensors_to_log = {\"probabilities\": \"sigmoid_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "    \n",
    "    STEP_SIZE = int(FLAGS.step_size)\n",
    "    \n",
    "    RNN_classifier.train(\n",
    "        input_fn= lambda: train_input_fn(x_train,y_train,batch_size=100),\n",
    "        steps=STEP_SIZE,\n",
    "        hooks=[logging_hook])  \n",
    "\n",
    "    eval_results = RNN_classifier.evaluate(\n",
    "       input_fn = lambda: eval_input_fn(x_test,y_test,batch_size=100))\n",
    "    \n",
    "    print(eval_results)\n",
    "    \n",
    "    # Save the model\n",
    "    RNN_classifier.export_savedmodel(FLAGS.saved_dir, serving_input_receiver_fn=serving_input_receiver_fn)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
